---
title: "Credit Card Default"
author: "Dareck Giuliani, Jacob Summerhays, and Sam Wilson"
date: "12/2/2020"
output:
  html_document:
    toc: yes
---

# Define

Primarily, we intend to create a model that predicts default rate. We will also answer the following questions:

1. How does the probability of default payment vary by categories of different demographic variables?
2. Which variables are the strongest predictors of default payment?

# Collect

The following libraries will be used:

```{r setup, include=FALSE}

knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
library(tidyverse) #for manipulating and visualizing data
library(MASS) #has function stepAIC(), which allows for easy stepwise regression
library(psych) #dummy code function
library(fastDummies) #dummy code function
library(caret) #for computing cross-validation methods
library(pROC) #to create ROC curves
library(ROCR) #an alternate ROC curve package
library(class) #for the knn() function
library(randomForest) #for the random forest classifer
library(gbm) #for GBM function
library(gdata) #to rename columns 
select <- dplyr::select

options(scipen=999) #effectively eliminates scientific notation

```

## The Raw Data

This dataset contains information about default payments, demographic factors, credit data, history of payment, and bill statements of credit card clients in Taiwan from April 2005 to September 2005.

### Variables

#### Independent Variables

* ID: ID of each client
* LIMIT_BAL: Amount of given credit in NT dollars (includes individual and family/supplementary credit
* SEX: Gender (1=male, 2=female)
* EDUCATION: (1=graduate school, 2=university, 3=high school, 4=others, 5=unknown, 6=unknown)
* MARRIAGE: Marital status (1=married, 2=single, 3=others)
* AGE: Age in years
* PAY_0: Repayment status in September, 2005 (-1=pay duly, 1=payment delay for one month, 2=payment delay for two months, â€¦ 8=payment delay for eight months, 9=payment delay for nine months and above)
* PAY_2: Repayment status in August, 2005 (scale same as above)
* PAY_3: Repayment status in July, 2005 (scale same as above)
* PAY_4: Repayment status in June, 2005 (scale same as above)
* PAY_5: Repayment status in May, 2005 (scale same as above)
* PAY_6: Repayment status in April, 2005 (scale same as above)
* BILL_AMT1: Amount of bill statement in September, 2005 (NT dollar)
* BILL_AMT2: Amount of bill statement in August, 2005 (NT dollar)
* BILL_AMT3: Amount of bill statement in July, 2005 (NT dollar)
* BILL_AMT4: Amount of bill statement in June, 2005 (NT dollar)
* BILL_AMT5: Amount of bill statement in May, 2005 (NT dollar)
* BILL_AMT6: Amount of bill statement in April, 2005 (NT dollar)
* PAY_AMT1: Amount of previous payment in September, 2005 (NT dollar)
* PAY_AMT2: Amount of previous payment in August, 2005 (NT dollar)
* PAY_AMT3: Amount of previous payment in July, 2005 (NT dollar)
* PAY_AMT4: Amount of previous payment in June, 2005 (NT dollar)
* PAY_AMT5: Amount of previous payment in May, 2005 (NT dollar)
* PAY_AMT6: Amount of previous payment in April, 2005 (NT dollar)

#### Dependent Variables

* default.payment.next.month: Default payment (1=yes, 0=no)


```{r read in data, echo=TRUE}

#read in data
creditDf <- read.csv('UCI_Credit_Card.csv')
#https://www.kaggle.com/uciml/default-of-credit-card-clients-dataset

#standardize column names
colnames(creditDf) <- tolower(make.names(colnames(creditDf)))

head(creditDf)

```

# Organize

## Cleaning the Data

```{r understanding the data, echo=TRUE}

#dimensions
dim(creditDf)

#missing values
sum(is.na(creditDf))

#summary
summary(creditDf)

# What is the ratio of defaults?
sum(creditDf$default.payment.next.month)/length(creditDf$default.payment.next.month)

```

### Correcting the Data Types

1. Alter Sex to be a 0-1 variable and ensure it is categorical. 
2. Education has one level for other and two levels for unknown. Each of these will be grouped together. 
3. Education had a few entries of 0, this will be grouped with other and unknown. 
3. Education is then made into a categorical variable. 
4. Marriage is made into a categorical variable. 
5. Marriage has a few entries of 0. These value has no meaning, it will be grouped with other. 
6. The dependent variable, credit default, it made into a categorical variable. 
7. Each of the pay variables is made into a categorical variable. 
8. Each of the pay variables has an entry of -2. This is an erroneous entry and instead the absolute value is used. 

### Feature Engineering

1. An average is taken of all the bill amount variables. 
2. An average is taken of all the pay amount variables. 
3. A ratio of bill amount to pay amount is created. 
4. A variable for limit use is also created by dividing average bill over limit. 
5. All of the categorical variables are made into dummy variables. 

```{r correct date types, echo=TRUE}
creditDf <- creditDf %>%
  mutate(
    
    # Data Cleaning
    
    # Make sex a binary variable (1=female, 0 = male) and make it categorical.
    sex = as.factor(if_else(creditDf$sex == 2, true = 1, false = 0)),
    
    # Education contains these three repetitive levels: 4=others, 5=unknown, 6=unknown. This next line is to consolidate the the three levels into one level. Additionally, some entries have education labeled as 0. 0 is not a known level according to the given key. As such, 0 will be grouped with others and unknown. Education is made into a categorical variable as well. 
    education = as.factor(ifelse(creditDf$education > 4 | creditDf$education == 0, 4, creditDf$education)),
    
    # Some entries of marriage are labeled as 0. However, the key given to us, does not provide meaning for 0. Thus, values for marriage of 0 will be grouped into the other bin. 
    #make marriage a categorical variable
    marriage = as.factor(ifelse(creditDf$marriage == 0, 3, creditDf$marriage)),
    
    # The dependent variable is made into a categorical variable (1 = yes, 0 = no). 
    default.payment.next.month = as.factor(creditDf$default.payment.next.month),
    
    # The pay variables have values of -2. However, -2 is never provided in the key. I am imputing the absolute value instead. 
    pay_0 = ifelse(creditDf$pay_0 == -2, 2, creditDf$pay_0),
    pay_2 = ifelse(creditDf$pay_2 == -2, 2, creditDf$pay_2),
    pay_3 = ifelse(creditDf$pay_3 == -2, 2, creditDf$pay_3),
    pay_4 = ifelse(creditDf$pay_4 == -2, 2, creditDf$pay_4),
    pay_5 = ifelse(creditDf$pay_5 == -2, 2, creditDf$pay_5),
    pay_6 = ifelse(creditDf$pay_6 == -2, 2, creditDf$pay_6),
    
    # Each pay variable is made into a categorical variable. 
    pay_0 = as.factor(creditDf$pay_0),
    pay_2 = as.factor(creditDf$pay_2),
    pay_3 = as.factor(creditDf$pay_3),
    pay_4 = as.factor(creditDf$pay_4),
    pay_5 = as.factor(creditDf$pay_5),
    pay_6 = as.factor(creditDf$pay_6),
    

    
    # Feature Engineering
    
    bill_avg = round((creditDf$bill_amt1 + creditDf$bill_amt2 + creditDf$bill_amt3 + creditDf$bill_amt4 + creditDf$bill_amt5 + creditDf$bill_amt6)/6, 2),
    
    pay_avg = round((creditDf$pay_amt1 + creditDf$pay_amt2 +creditDf$pay_amt3 + creditDf$pay_amt4 + creditDf$pay_amt5 + creditDf$pay_amt6 ) /6,2),
    
    #Credit scores often judge a person on their credit usage. The closer to the limit a person is, the more of a negative impact on credit score. This metric will divide the average bill amount by the limit balance to get a sense of the average credit usage.
    avg_credit_usage = round(bill_avg/limit_bal, 2),

    
  ) %>%
  
  # ID is dropped as it will have no predictive power. 
  select(-c(id))

# This will show the data type of each variable within the data frame. 
sapply(creditDf, class)

# Additionally, I'd like to see some basic statistics of the cleaned data.
summary(creditDf)

```


```{r Udeful Subsets, echo=TRUE}

# Useful Subsets

# This creates a data frame of all categorical variables.
categorical <- select(creditDf, c(sex, education, marriage, default.payment.next.month, pay_0, pay_2, pay_3, pay_4, pay_5, pay_6))

# This creates a data drame of all continuous variables.
continuous <- select(creditDf, -c(sex, education, marriage, default.payment.next.month, pay_0, pay_2, pay_3, pay_4, pay_5, pay_6))

# This dummy codes all categorical variables. 
dummy <- select(creditDf, c(education, marriage, pay_0, pay_2, pay_3, pay_4, pay_5, pay_6))

creditDf_Dummy <- dummy_cols(creditDf, select_columns = colnames(dummy))

# The following lines of codes create a data frame with scaled values for the continuous variables. 
creditDf_Scaled <- creditDf %>%
  select(-c(sex, education, marriage, default.payment.next.month, pay_0, pay_2, pay_3, pay_4, pay_5, pay_6)) %>%
  scale() %>%
  data.frame()

creditDf_Scaled <- cbind(creditDf_Scaled, categorical)

creditDf_Scaled <- rename.vars(creditDf_Scaled, from = dput(names(continuous)), to = paste0('scaled.', dput(names(continuous))))

# The following creates testing and training data sets.

# Creates a sample of 80% of the data
sample_size <- floor(.8 * nrow(creditDf))

#
set.seed(123)
train_index <- sample(seq_len(nrow(creditDf)), size = sample_size)

# This is a training and testing set with no scaled data. 
train <- creditDf[train_index, ]
test <- creditDf[-train_index, ]

# This is a training and testing set with scaled data. 
train_scaled <- creditDf_Scaled[train_index, ]
test_scaled <- creditDf_Scaled[-train_index, ]


```

## Descriptive Statistics

# Visualize

To begin, the below is a visualization of the ratio of default to no defaults.

```{r default rate, echo=TRUE}

creditDf %>%
  ggplot(aes(x = default.payment.next.month)) +
  geom_bar() +
  ylab('Count') +
  xlab('Default (1 = yes)') +
  ggtitle('The Number of Defaults in the Dataset') +
  theme_minimal()

```

Now, we examine the relationship between demographic variables and defaulting. 

```{r visualize demographic, echo=TRUE, message=FALSE, warning=FALSE}

# 1 = Female, 0 = Male
sex.label <- c('Female', 'Male')
names(sex.label) <- c('1', '0')
# 1 = Married, 2 = Single, 3 = Other
marriage.label <- c('Married', 'Single', 'Other')
names(marriage.label) <- c('1', '2', '3')

creditDf %>%
  group_by(sex, marriage, education, default.payment.next.month) %>%
  tally() %>%
  mutate(
    proportion = n/sum(n)
  )  %>%
  ggplot(aes(x = education, y = proportion, fill = default.payment.next.month)) +
  geom_col() +
  facet_wrap(~sex + marriage, 
             #labeller(sex = sex.label,
              #        marriage = marriage.label)
             ) +
  xlab('Education') +
  ylab('Proportion of Defaults') +
  labs(fill = 'Default 1 = yes | 0 = no') +
  ggtitle('The Proportion of Default by Education, Sex, and Marriage') +
  theme_minimal() +
  theme(
    legend.position = 'bottom',
    axis.text.x = element_text(angle = 45, hjust = 1)
  ) +
  scale_x_discrete(labels  = c('1' = 'Graduate School', '2' = 'Some University', '3' = 'High School', '4' = 'Other/Unknown'), 
                   limits = seq(1, 4)
                   )

creditDf %>%
  filter(age < 75) %>%
  group_by(age, sex, default.payment.next.month) %>%
  tally() %>%
  mutate(
    proportion = n/sum(n)
  ) %>%
  ggplot(aes(x = age, y = proportion, fill = default.payment.next.month)) +
  geom_col() +
  facet_wrap(~ sex#, 
             #labeller(sex = sex.label)
             ) +
  xlab('Age') +
  ylab('Proportion') +
  ggtitle('The Proportion of Default by Age and Sex') +
  labs(fill = 'Default 1 = yes | 0 = no') +
  theme_minimal() +
  theme(
    legend.position = 'bottom'
  )

```
```{r Bill Amt vs. Default, echo=TRUE}

creditDf %>%
  select( bill_amt6, bill_amt5, bill_amt4, bill_amt3, bill_amt2, bill_amt1, default.payment.next.month) %>%
  gather(-default.payment.next.month, key = 'iv', value = 'value') %>%
  ggplot(aes(x = value,  fill = default.payment.next.month)) +
  geom_density() +
  facet_wrap(~iv) +
  xlim(0, 75000) +
  ylab('Density') +
  xlab('Amount of Bill Statement ($)') +
  labs(fill = 'Default 1 = yes | 0 = no') +
  theme_minimal() +
  theme(
    legend.position = 'bottom',
    axis.text.x = element_text(angle = 45, hjust = 1)
  ) 


```

```{r Pay Amt vs. Default, echo=TRUE}

creditDf %>%
  select( pay_amt6, pay_amt5, pay_amt4, pay_amt3, pay_amt2, pay_amt1, default.payment.next.month) %>%
  gather(-default.payment.next.month, key = 'iv', value = 'value') %>%
  ggplot(aes(x = value,  fill = default.payment.next.month)) +
  geom_density() +
  facet_wrap(~iv) +
  xlim(0, 10000) +
  ylab('Density') +
  xlab('Amount of Previous Payment ($)') +
  labs(fill = 'Default 1 = yes | 0 = no') +
  theme_minimal() +
  theme(
    legend.position = 'bottom',
    axis.text.x = element_text(angle = 45, hjust = 1)
  ) 

```


This is a histogram of each of the variables. 

```{r IV box plots, echo=TRUE}

creditDf %>% 
  sample_n(50) %>% #samples from the data set to improve speed
  gather(-default.payment.next.month, key = 'iv', value = 'value') %>%
  ggplot(aes(y = value,  group = default.payment.next.month, x = default.payment.next.month)) +
  geom_boxplot() +
  facet_wrap(~iv, scales = 'free') +
  ggtitle('Histograms of the Variables') +
  theme_minimal() +
  theme(
    axis.text.y = element_blank(),
    axis.title.x = element_blank(),
    axis.title.y = element_blank()
  ) 
  

```
The next code block creates histograms and density plots for the categorical and continuous variables, respectively. 

```{r histograms, echo=TRUE}

#histograms of categorical data
categorical %>%
  sample_n(30) %>%
  gather(key = 'var', value = 'value') %>%
  ggplot(aes(x = value)) +
  geom_histogram(stat = 'count') +
  facet_wrap(~var, scales = 'free') +
  xlab('Variable') +
  ylab('Count') +
  ggtitle('Histograms of All Categorical Variables') +
  theme_minimal() +
  theme(
    #axis.text.x = element_text(angle = 45, hjust = 1),
    axis.text.y = element_blank(),
    axis.title.x = element_blank()
  )

#histograms of continuous data
continuous %>%
  sample_n(30) %>%
  gather(key = 'var', value = 'value') %>%
  ggplot(aes(x = value)) +
  geom_density() +
  facet_wrap(~var, scales = 'free') +
  ylab('Density') +
  ggtitle('Density Plots of All Continuous Variables') +
  theme_minimal() +
  theme(
    #axis.text.x = element_text(angle = 45, hjust = 1),
    axis.text.x = element_blank(),
    axis.text.y = element_blank(), 
    axis.title.x = element_blank(),
    
  )
  
  

```

The below code chunk visualizes two variables and the groups the data by whether or not they defaulted. 

```{r Scatter plots, echo=TRUE}

#Average Bill Amount vs. Average Pay Amount
creditDf %>%
  ggplot(aes(x = bill_avg, y = pay_avg, color = default.payment.next.month)) +
  geom_point() +
  theme_minimal() +
  theme(
    legend.position = 'bottom'
  )

#Age vs. Average Bill Amount
creditDf %>%
  ggplot(aes(x = age, y = pay_avg, color = default.payment.next.month)) +
  geom_point() +
    theme_minimal() +
  theme(
    legend.position = 'bottom'
  )

creditDf %>%
  ggplot(aes(x = age, y = bill_avg, color = default.payment.next.month)) +
  geom_point() +
    theme_minimal() +
  theme(
    legend.position = 'bottom'
  )

creditDf %>%
  ggplot(aes(x = avg_credit_usage, fill = default.payment.next.month)) +
  geom_bar() +
    theme_minimal() +
  theme(
    legend.position = 'bottom'
  )


```

The below chunk of code visualizes the demographic variables and their relation to defaulting. 

```{r Default v Demo Info, echo=TRUE}

#Limit Balance
creditDf_LB <- creditDf %>% group_by(limit_bal, default.payment.next.month) %>% summarise(Counts = n())
head(creditDf_LB)
names(creditDf_LB) <- c("LIMIT_BAL","default","Counts") 
lbdist = ggplot(creditDf_LB, aes(x=factor(LIMIT_BAL), y=Counts, fill=factor(default))) + 
      geom_bar(stat="identity") + 
      xlab('Limit Balanace')+
      ylab('Total')+
      theme_minimal()+
      ggtitle("Limit Balance Distribution") +
      theme(
        axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = 'bottom'
      )
lbdist = lbdist + guides(fill=guide_legend(title ='Default Payment: 0 = No Default | 1 = Default' )) ; lbdist
##The customers who have lower limit balance have much more higher default rate.

#Gender
creditDf_Sex <- creditDf %>% group_by(sex, default.payment.next.month) %>% summarise(Count = n())
head(creditDf_Sex)
names(creditDf_Sex) <- c("SEX","default","Count") 
genderdist = ggplot(creditDf_Sex, aes(x=factor(SEX), y=Count, fill=factor(default))) + 
      geom_bar(stat="identity") + 
      xlab('Gender: 0 = Male | 1 = Female')+
      ylab('Total')+
      theme_minimal()+
      ggtitle("Gender Distribution") +
      theme(
        legend.position = 'bottom'
      )
genderdist = genderdist + guides(fill=guide_legend(title ='Default Payment: 0 = No Default | 1 = Default' )) ; genderdist
##The customers who are defined as 'Female' have a lower default rate.

#Education
creditDf_Edu <- creditDf %>% group_by(education, default.payment.next.month) %>% summarise(Count = n())
head(creditDf_Edu)
names(creditDf_Edu) <- c("EDU","default","Count") 
edudist = ggplot(creditDf_Edu, aes(x=factor(EDU), y=Count, fill=factor(default))) + 
      geom_bar(stat="identity") + 
      xlab('Education: 1 = Grad School, 2 = Under Grad, 3 = High School, 4 = Others')+
      ylab('Total')+
      theme_minimal()+
      ggtitle("Educatiton Distribution")
edudist = edudist + guides(fill=guide_legend(title ='Default Payment: 0 = No Default | 1 = Default' )) ; edudist
##The Customers who have competed undergrad have a higher default rate.

#Marriage
creditDf_Mar <- creditDf %>% group_by(marriage, default.payment.next.month) %>% summarise(Count = n())
names(creditDf_Mar) <- c("MAR","default","Count")
creditDf_Mar = creditDf_Mar %>% filter(MAR != 0)
mardist = ggplot(creditDf_Mar, aes(x=factor(MAR), y=Count, fill=factor(default))) + 
      geom_bar(stat="identity") + 
      xlab('Martial Status: 1 = Married, 2 = Single, 3 = Others')+
      ylab('Total')+
      theme_minimal()+
      ggtitle("Marriage Distribution") +
      theme(
        legend.position = 'bottom'
      )
mardist = mardist + guides(fill=guide_legend(title ='Default Payment: 0 = No Default | 1 = Default' )) ; mardist
##Married and Single Customers have a apprx the same default rate.

#Age
creditDf_Age <- creditDf %>% group_by(age, default.payment.next.month) %>% summarise(Count = n())
head(creditDf_Age)
names(creditDf_Age) <- c("age","default","Count") 
agedist = ggplot(creditDf_Age, aes(x=age, y=Count, fill=factor(default))) + 
      geom_bar(stat="identity") + 
      xlab('Age')+
      ylab('Total')+
      theme_minimal()+
      ggtitle("Age Distribution") +
      theme(
        legend.position = 'bottom'
      )
agedist = agedist + guides(fill=guide_legend(title ='Default Payment: 0 = No Default | 1 = Default' )) ; agedist
##Younger customers have a higher default rate.



```



```{r Correaltions, echo=TRUE, echo=TRUE}
#lares::corr_cross(creditDf, type = 1)
#lares::corr_cross(creditDf, type = 1, ignore = c('pay_0', 'pay_2', 'pay_3', 'pay_4', 'pay_5', 'pay_6', 'bill_amt1', 'bill_amt2', 'bill_amt3', 'bill_amt4', 'bill_amt5','bill_amt6', 'pay_amt1', 'pay_amt2', 'pay_amt3', 'pay_amt4', 'pay_amt5','pay_amt6'))
#pairs(ccDate_demoinfo)
##Off initial review of core data (i.e. not payment data) limit balance, avg credit usage, and payment avg seem to have the largest correlation to whether or not a customer will default
```

# Analyze

# Classification Models

## Logistic Regression

### Computing Stepwise Logistic Regression


Because of the size of the data set, it is impractical to train a logistic regression with every variable. Instead logistic models are trained with groups of variables. 

The first block of code uses only the demographic information. 

```{r Stepwise Logistic Demographics}

#logit_model1 <- glm(default.payment.next.month ~ . , data = train, family = 'binomial') %>%
#  stepAIC(trace = FALSE)
#After 30 minutes, R continued to process

#Create a logistic model using only demographic variables. 
logit_modelDemographics <- glm(default.payment.next.month ~ sex + education_1 +education_2 + education_3 + education_4
                               + marriage_1 + marriage_2 + marriage_3 
                              , data = train, family = 'binomial') #%>%
  #stepAIC(trace = FALSE)

#Summarize the results. 
summary(logit_modelDemographics)

#Then, we will predict how well the logistic regression performs. 
#test$demo.model.probability <- predict(logit_modelDemographics, test, type= "response")


```
This block only uses the bill_amt variables. 

```{r Logistic Bill Amount, echo=TRUE}

logit_modelBillAmt <- glm(default.payment.next.month ~ bill_amt1 +  bill_amt2 + bill_amt3 + bill_amt4 + bill_amt5 + bill_amt6,
                          data = train, family = 'binomial')

#Only Bill_Amt1 has a significant p-value
summary(logit_modelBillAmt)

```

This block of code only uses the pay_amt variables.

```{r Logit Pay Amount, echo=TRUE}

logit_modelPayAmt <- glm(default.payment.next.month ~ pay_amt1 + pay_amt2 + pay_amt3 + pay_amt4 + pay_amt5 + pay_amt6,
                         data = train, family = 'binomial')

summary(logit_modelPayAmt)

```
The remaining variables related to payment are used. 

```{r Logit Other Pay Variables, echo=TRUE}

logit_modelOtherPay <- glm(default.payment.next.month ~ limit_bal + bill_avg + pay_avg + avg_credit_usage,
                     data = train, family = 'binomial')

summary(logit_modelOtherPay)

#interaction between gender and marriage. Are make singles more likely to default?

```
The significant variables from above are used to train two models. 

```{r Stepwise Logistic}

# Model 1
logit_model1 <- glm(default.payment.next.month ~ sex + education +
                      bill_amt1 + 
                      pay_amt1 + pay_amt2 + pay_amt3 + pay_amt4 + pay_amt5 + pay_amt6 +
                      limit_bal +  avg_credit_usage,
                    data = train, family = 'binomial')

summary(logit_model1)


# Model 2
logit_model2 <- glm(default.payment.next.month ~ sex + education +
                      pay_amt1 + pay_amt2 + pay_amt3 + pay_amt4 + pay_amt5 +
                      limit_bal +  avg_credit_usage,
                    data = train, family = 'binomial')

summary(logit_model2)

# Model 3A - This model will use the scaled data.
logit_model3 <- glm(default.payment.next.month ~ limit_bal + age + 
                      bill_amt1 + bill_amt2 + bill_amt3 + bill_amt4 + bill_amt5 + bill_amt6 +
                      pay_amt1 + pay_amt2 + pay_amt3 + pay_amt4 + pay_amt5 + pay_amt6 +
                      bill_avg + pay_avg + avg_credit_usage,
                    data = train_scaled, family = 'binomial')

summary(logit_model3)

# Model 3B

logit_model3B <- glm(default.payment.next.month ~ scaled.limit_bal + scaled.age + 
                    scaled.pay_amt1 + scaled.avg_credit_usage,
                    data = train_scaled, family = 'binomial')

summary(logit_model3B)

```
### Making Predictions with the Logistic Models

```{r logit prediction, echo=TRUE}

#Model 1

#make the prediction
test$logit.model1.predict <- predict(logit_model1, test, type = 'response')

#create a ROC curve
plot(
  roc(default.payment.next.month ~ logit.model1.predict, data = test),
)

#determines the best cut-off
coords(roc(default.payment.next.month ~ logit.model1.predict, data = test), 'best')

#uses the best cut-off to classify predictions
test$logit.model1.binary <- ifelse(test$logit.model1.predict >= 0.2283391, 1, 0)

test$logit.model1.binary  <- as.factor(test$logit.model1.binary )

#creates a confusion matrix
confusionMatrix(test$logit.model1.binary, test$default.payment.next.month)

#model 2

#make a prediction
test$logit.model2.predict <- predict(logit_model2, test, type = 'response')

#create a ROC curve
plot(
  roc(default.payment.next.month ~ logit.model2.predict, data = test),
)

# find the best cut-off
coords(roc(default.payment.next.month ~ logit.model2.predict, data = test), 'best')

# implement the cut-off
test$logit.model2.binary <- ifelse(test$logit.model1.predict >= 0.2273121, 1, 0)

test$logit.model2.binary  <- as.factor(test$logit.model2.binary)

# create a confusion matrix
confusionMatrix(test$default.payment.next.month, test$logit.model2.binary)

#These models are practically the same and are not very predictive. 

#Accuracy

for (i in 1:nrow(test)) {
  model1.correct[i] <- ifelse(test$logit.model1.binary[i] == test$default.payment.next.month[i], 1, 0)
  model2.correct[i] <- ifelse(test$logit.model2.binary[i] == test$default.payment.next.month[i], 1, 0)
}

logit1.accuracy <- (sum(model1.correct)/length(model1.correct) * 100)
logit2.accuracy <- (sum(model2.correct)/length(model1.correct) * 100)


```



## k-Nearest Neighbors

https://towardsdatascience.com/k-nearest-neighbors-algorithm-with-examples-in-r-simply-explained-knn-1f2c88da405c

The data needed to be standardized and training as well as test sets need to be created. 

```{r standardize, echo=TRUE}

#standardize the continuous data 
creditDf_Scaled <- data.frame(scale(continuous))

creditDf_Scaled <- cbind(creditDf_Scaled, creditDf$default.payment.next.month)

creditDf_Scaled <- creditDf_Scaled %>%
  rename(
      default.payment.next.month = `creditDf$default.payment.next.month`
  )

#creates a sample of 80% of the data
sample_size <- floor(.8 * nrow(creditDf_Scaled))

#
set.seed(456)
train_index <- sample(seq_len(nrow(creditDf_Scaled)), size = sample_size)

#create a training a testing data set
train <- creditDf_Scaled[train_index, ]
test <- creditDf_Scaled[-train_index, ]

#extract the target column
credit_target_category <- train$default.payment.next.month
credit_test_category <- test$default.payment.next.month


```



```{r knn, echo=TRUE}

#run knn
prediction <- knn(train = train, test = test, cl = train$default.payment.next.month, k = 5)

#create a confusion matrix
confusion_matrix <- table(prediction, credit_test_category)

confusion_matrix

accuracy <- function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}

accuracy.knn <- accuracy(confusion_matrix)

accuracy.knn 

```

This accuracy is insanely high, there must be target leakage. 


## Random Forest 

https://www.blopig.com/blog/2017/04/a-very-basic-introduction-to-random-forests-using-r/

### Testing and Training Data Set

```{r train test split rf, echo=TRUE}

#creates a sample of 80% of the data
sample_size <- floor(.8 * nrow(creditDf))

#
set.seed(678)
train_index <- sample(seq_len(nrow(creditDf)), size = sample_size)

#
train <- creditDf[train_index, ]
test <- creditDf[-train_index, ]

# Drop the ID column from both the train and test set.
train <- select(train, -c(id))
test <- select(test, -c(id))


```

### Train the Random Forest Model

```{r train rf, echo=TRUE}

# Model 1
rf_classifer = randomForest(default.payment.next.month ~ ., data = train, ntree = 100, mtry = 2, importance = TRUE)

rf_classifer

varImpPlot(rf_classifer)

# Model 2
rf_classifer_2 = randomForest(default.payment.next.month ~ pay_0 + pay_2 + pay_3 + pay_4 + pay_4 +
                                bill_amt6 + bill_amt5 + bill_amt4 + bill_amt3 + bill_amt2 + bill_amt1 + avg_credit_usage +
                                pay_amt6 + pay_amt5 + pay_amt4 + pay_amt3 + pay_amt2 + pay_amt1 +
                                limit_bal
                                , data = train, ntree = 100, mtry = 2, importance = TRUE)

rf_classifer_2

varImpPlot(rf_classifer_2)

#There is hardly any reduction in the OOB estimate in model 2. From 18.62% to 18.52%.

```

### Predict with the Random Forest Classifer

```{r predict rf, echo=TRUE}

rf_prediction <- predict(rf_classifer_2, test)

table(observed = test$default.payment.next.month, predicted = rf_prediction)

rf_prediction_prob <-  predict(rf_classifer_2, test, type = 'prob')

true_value <- ifelse(test$default.payment.next.month == rf_prediction, 1, 0)

pred <- prediction(rf_prediction_prob[, 1], true_value)

perf <- performance(pred, 'tpr', 'fpr')

plot(perf)

auc.perf <- performance(pred, measure = 'auc')

accuracy.rf <- as.numeric(auc.perf@y.values) * 100

print(auc.perf@y.values)



```


## GBM Model
Gradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. It builds the model in an iterative fashion like other boosting methods do, and it generalizes them by allowing optimization of an arbitrary differentiate loss function. 

https://koalaverse.github.io/machine-learning-in-R/gradient-boosting-machines.html

```{r GBM Prediction, echo=TRUE, message=FALSE, warning=FALSE}
# Partition into training and test data
set.seed(456)
indexGBM <- createDataPartition(creditDf$default.payment.next.month, p = 0.7, list = FALSE)
train_dataGBM  <- creditDf[indexGBM , ]
test_dataGBM   <- creditDf[-indexGBM , ]

# Train model with preprocessing & repeated cv
model_gbm <- caret::train(default.payment.next.month ~ .,
                          data = train_dataGBM,
                          method = "gbm",
                          preProcess = c("scale", "center"),
                          trControl = trainControl(method = "repeatedcv", 
                                                  number = 5, 
                                                  repeats = 3, 
                                                  verboseIter = FALSE),
                          verbose = 0, na.action=na.exclude)
model_gbm

# Test GBM Model against Test Data
pred.int <- predict(model_gbm, test_dataGBM, interval = "prediction")#; pred.int
gbm_data_frame <- cbind(test_dataGBM$default.payment.next.month, pred.int)#; gbm_data_frame
gbm_data_frame = as.data.frame(gbm_data_frame)#; gbm_data_frame


for (i in 1:nrow(gbm_data_frame)) {
  gbm.correct[i] <- ifelse(gbm_data_frame$V1[i] == gbm_data_frame$pred.int[i], 1, 0)
}

accuracy.gbm <- sum(gbm.correct)/length(gbm.correct) * 100

```

## Model Comparison (Accuracy)

```{r accuracy comparison, echo=TRUE}

model.comparison <- data.frame(
  
  Model = c('Logistic Model 1', 'Logistic Model 2', 'KNN Model', 'Random Forest', 'GMB Model'),
  Accuracy = c(round(logit1.accuracy, 3), round(logit2.accuracy, 3), round(accuracy.knn, 3), round(accuracy.rf,3), round(accuracy.gbm, 3))
  
)


model.comparison

model.comparison %>%
  ggplot(aes(x = Model, y = Accuracy)) +
  geom_col() +
  ggtitle('Accuracy of the Models') +
  theme_minimal()

```

## Cross Validation

## Conclusion

# Sources